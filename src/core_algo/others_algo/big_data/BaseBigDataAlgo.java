package core_algo.others_algo.big_data;

// TODO: 在大数据场景下，如何找到优化的点，算法如何体现 ?
// 1. 在处理大数据的场景时，考虑不将所有的数据元素进行存储，而是存储数据的统计信息或者算法信息 !!
// 2. 使用hash来运算和提取结果     ==> 时间复杂度
// 3. 使用存储量最小的数据结构(bit) ==> 空间复杂度
public class BaseBigDataAlgo {

    // 1. 随机均匀数据流采样
    //    无限的整数数据流，如何从中随机地抽取k个整数出来 ?
    //    所谓随机，指的是每个元素被抽中的概率都是一样的(1/n)
    //    对于一个新的元素，在一定的概率下才被使用或舍弃
    //    1.2 当k=1，只抽取一个元素时
    //        当第i个整数到达时，以1/i的概率使用第i个整数替换被选中的整数，以1−(1/i)的概率丢弃第i个整数
    //    1.3 当k>1，抽取多个元素时
    //        第i个整数到达时，以k/i的概率替换k个数中的某一个，以1−(k/i)的概率丢弃，保留k个数不变

    // 2. 基数估计(Cardinality Estimation)/独立访客统计(Unique Visitor，简称UV)
    //    计算数据流中有多少种不同的元素组成？
    //    2.1 HashSet<T> 具有单机内存的限制
    //    2.2 Bitmap 长度为N的bit数组，元素与位置一一对应(不需要存储元素值本身)
    //    2.3 Linear Counting
    //    2.4 LogLog Counting
    //    2.5 HyperLogLog Counting 专门做基数统计的数据结构

    // -------------------------------------------------------------------------
    // 3. 频率估计(Frequency Estimation)
    //    计算数据流中任意元素的频率(出现的次数) ?
    //    3.1 HashMap<K,V> 在大数据场景下，单机存储的限制
    //    3.2 负载均衡+HashMap<K,V> 对数据进行分片，存储到多台机器上
    //    3.3 Count-Min Sketch(不需要存储元素值本身) 使用多个hash方法来统计，最终判断时取所有统计值中的最小值
    //        由于hash冲突的存在，必然会导致错误概率

    // 4. 数据流Top K最频繁(Heavy Hitters)
    //    取出数据流中频率最高的K个元素 ?
    //    4.1 TODO: Quick Select(快速选择算法)单机模式很难解决大数据场景
    //    4.2 多机HashMap + Heap ==> 和"Top10_VisitedIps"同样的解法
    //    4.3 Count-Min Sketch(不需要存储元素值本身) + Heap
    //    4.4 Lossy Counting(有损计数)，去掉统计意义上的元素，保存占用的内存空间在一个很小的范围
    //        出现频率高的元素，不太可能减一后变成0 ==> 存在一定的错误率

    // -------------------------------------------------------------------------
    // 5. 范围查询(Range Query)
    //    无限的整数数据流，如何查询在某个范围内的元素出现的总次数 ?
    //    SELECT count(v) WHERE v >= l AND v < u; 需要计算范围中每个元素的次数，然后求和
    //    5.1 Count-Min Sketch算法和数据结构

    // 6. 成员查询(Membership Query)
    //    定一个无限的数据流和一个有限集合，如何判断数据流中的元素是否在这个集合中 ?
    //    6.1 TODO: 先将有限集合中的原始构建成布隆过滤器(Bloom Filter)(不需要存储元素值本身)
    //    6.2 对于数据流中的任何一个元素，直接通过Bloom Filter判断是否存在
    //    6.3 元素的判断存在容错概率(取决于bit位数组的长度和hash函数的方法)
}